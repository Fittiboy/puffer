{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac974d59-2a7c-4d54-b7ec-39633bed604b",
   "metadata": {},
   "source": [
    "# Testing REINFORCE with Puffer-vectorized Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0c3a0-ac4d-4cc0-bca4-cf6dcb6e7b8e",
   "metadata": {},
   "source": [
    "### Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b350f1c9-d5cf-419b-955b-6cc53ce6db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pufferlib\n",
    "import pufferlib.vector\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from pufferlib.environments import atari\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b870e5-a3c8-4e70-b8b6-df70d1071e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9208ac4-1f70-4d46-ade8-c06fb7ef1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers, for four stacked grayscale 84x84 frames\n",
    "        self.conv1 = nn.Conv2d(in_channels= 4, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        conv_output_size = 16 * 6 * 5\n",
    "\n",
    "        # Fully connected head\n",
    "        self.fc = nn.Linear(conv_output_size, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x / 255.0\n",
    "        x = torch.relu(F.max_pool2d(self.conv1(x), 2, 2))\n",
    "        x = torch.relu(F.max_pool2d(self.conv2(x), 2, 2))\n",
    "        x = torch.relu(F.max_pool2d(self.conv3(x), 2, 2))\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156deb37-b7e2-4bad-ad39-4d88dc3120b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredPolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(121, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b47c179-3f47-4677-9eea-f0e521450e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_actions(obs):\n",
    "    states = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "    logits = policy_net(states)\n",
    "\n",
    "    # Create the distribution and sample from it\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    actions = dist.sample()\n",
    "\n",
    "    # We need the log probability for gradient computation\n",
    "    log_probs = dist.log_prob(actions)\n",
    "\n",
    "    # Get the entropy for logging, possibly as a bonus to the loss\n",
    "    entropies = dist.entropy()\n",
    "\n",
    "    return actions.cpu(), log_probs, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038415d7-a4f7-45dd-8aa0-b3be912c420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    policy: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    env,\n",
    "    update_batches: int = 16,\n",
    "    gamma: float = 0.99,\n",
    "    entropy_coef: float = 0.01,\n",
    "    num_steps: int = 1_000_000,\n",
    "    report_frequency: int = 50,\n",
    "):\n",
    "    try:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        log_probs_list = [[] for _ in range(obs.shape[0])]\n",
    "        entropies_list = [[] for _ in range(obs.shape[0])]\n",
    "        rewards_list = [[] for _ in range(obs.shape[0])]\n",
    "\n",
    "        total_episodes = 0\n",
    "        episodes = 0\n",
    "        total_return = 0.0\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for steps in tqdm(range(1, num_steps + 1), desc=\"Steps\"):\n",
    "            # Use the policy network for action selection\n",
    "            actions, log_probs, current_entropies = select_actions(obs)\n",
    "\n",
    "            # Step the environment with the chosen action\n",
    "            obs, rewards, dones, truncateds, _ = env.step(actions)\n",
    "\n",
    "            # Store log probs and rewards for loss calculation\n",
    "            for i in range(len(rewards)):\n",
    "                log_probs_list[i].append(log_probs[i])\n",
    "                entropies_list[i].append(current_entropies[i])\n",
    "                rewards_list[i].append(rewards[i])\n",
    "\n",
    "            # Handle completed episodes\n",
    "            done_indices = np.where(dones | truncateds)[0]\n",
    "            new_episodes = len(done_indices)\n",
    "            \n",
    "            for ep_i, done_i in enumerate(done_indices):\n",
    "                if episodes + ep_i < update_batches:\n",
    "                    T = len(rewards_list[done_i])\n",
    "                    \n",
    "                    # Compute the return G_t for each time step t\n",
    "                    # and compute the policy gradient loss\n",
    "                    ret = 0.0\n",
    "                    loss = 0.0\n",
    "                    for t in reversed(range(T)):\n",
    "                        ret = rewards_list[done_i][t] + gamma * ret\n",
    "                        loss -= log_probs_list[done_i][t] * ret - entropies_list[done_i][t] * entropy_coef\n",
    "\n",
    "                    loss /= T\n",
    "                    total_loss += loss\n",
    "                    total_return += ret\n",
    "\n",
    "                    if wandb.run is not None:\n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                \"episodes\": total_episodes + ep_i,\n",
    "                                \"ep_length\": T,\n",
    "                                \"return\": ret,\n",
    "                                \"total_reward\": sum(rewards_list[done_i]),\n",
    "                                \"loss\": loss.item(),\n",
    "                            },\n",
    "                            step=total_episodes + ep_i\n",
    "                        )\n",
    "                    \n",
    "                    log_probs_list[done_i] = []\n",
    "                    entropies_list[done_i] = []\n",
    "                    rewards_list[done_i] = []\n",
    "    \n",
    "            total_episodes += new_episodes\n",
    "            episodes += new_episodes\n",
    "                    \n",
    "            if episodes >= update_batches:\n",
    "                avg_loss = total_loss / episodes\n",
    "                avg_return = total_return / episodes\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                avg_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                log_probs_list = [[] for _ in range(obs.shape[0])]\n",
    "                entropies_list = [[] for _ in range(obs.shape[0])]\n",
    "                rewards_list = [[] for _ in range(obs.shape[0])]\n",
    "\n",
    "                episodes = 0\n",
    "                total_return = 0.0\n",
    "                total_loss = 0.0\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining stopped manually.\")\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.unwatch()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0d471-23b4-40be-a70a-b9f7f510049f",
   "metadata": {},
   "source": [
    "## Configuring the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f38c00b-d8e7-4839-a6aa-cc80591a0985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.9.0+750d7f9)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Box(0, 255, (12, 80, 4, 105), uint8),\n",
       " MultiDiscrete([4 4 4 4 4 4 4 4 4 4 4 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = \"Puffer-Breakout-REINFORCE\"\n",
    "env_name = \"breakout\"\n",
    "gamma = 0.99\n",
    "update_batches = 16\n",
    "learning_rate = 1e-3\n",
    "entropy_coef = 1e-1\n",
    "\n",
    "config = {\n",
    "    \"env\": env_name,\n",
    "    \"algo\": \"REINFORCE\",\n",
    "    \"gamma\": gamma,\n",
    "    \"update_batches\": update_batches,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"entropy_coef\": entropy_coef,\n",
    "}\n",
    "\n",
    "env_creator = atari.env_creator(\"breakout\")\n",
    "vecenv = pufferlib.vector.make(\n",
    "    env_creator,\n",
    "    num_envs=12,\n",
    "    backend=pufferlib.vector.Multiprocessing,\n",
    "    env_kwargs={\"framestack\": 4},\n",
    ")\n",
    "\n",
    "vecenv.observation_space, vecenv.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3cef52-526e-4dd9-8069-cd3eb3713c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = vecenv.action_space.nvec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16871cdd-8ea6-4d9e-bc38-a13dc56e2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork().to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f4890-05c9-4694-b040-56698765ab02",
   "metadata": {},
   "source": [
    "## Initialize W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5160251-9593-4012-9884-c1844d0ef83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "track = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c1cb0e-e0c3-4f3e-a467-ad2124313891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfitti\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fitti/projects/puffer/wandb/run-20250214_190614-qabvd8bd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE/runs/qabvd8bd' target=\"_blank\">jovial-darling-82</a></strong> to <a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE' target=\"_blank\">https://wandb.ai/fitti/Puffer-Breakout-REINFORCE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE/runs/qabvd8bd' target=\"_blank\">https://wandb.ai/fitti/Puffer-Breakout-REINFORCE/runs/qabvd8bd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if track:\n",
    "    wandb.init(\n",
    "        project=project,\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f086cb55-5647-4d4a-a10a-3b6d0f6fae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(policy_net, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327df5a7-f9b1-41f5-bf30-3bad1333c051",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97549769-b5dc-4a35-afb4-14d595501e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6703d5e5fe4b1fb49eadee18c2d89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 11.58 GiB of which 18.69 MiB is free. Including non-PyTorch memory, this process has 11.36 GiB memory in use. Of the allocated memory 10.66 GiB is allocated by PyTorch, and 470.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvecenv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentropy_coef\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(policy, optimizer, env, update_batches, gamma, entropy_coef, num_steps, report_frequency)\u001b[0m\n\u001b[1;32m     21\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m steps \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Use the policy network for action selection\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     actions, log_probs, current_entropies \u001b[38;5;241m=\u001b[39m \u001b[43mselect_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Step the environment with the chosen action\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     obs, rewards, dones, truncateds, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mselect_actions\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect_actions\u001b[39m(obs):\n\u001b[1;32m      2\u001b[0m     states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 3\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Create the distribution and sample from it\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n",
      "File \u001b[0;32m~/.conda/envs/puffer/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/puffer/lib/python3.11/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/puffer/lib/python3.11/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x), \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x), \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/puffer/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/puffer/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/puffer/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/puffer/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 11.58 GiB of which 18.69 MiB is free. Including non-PyTorch memory, this process has 11.36 GiB memory in use. Of the allocated memory 10.66 GiB is allocated by PyTorch, and 470.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(policy_net, optimizer, vecenv, update_batches=update_batches, gamma=gamma, entropy_coef=entropy_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dd201-2167-42f3-a82e-892cc554b6f3",
   "metadata": {},
   "source": [
    "## Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4acad-3a00-4753-b4a2-3bfb9fa32c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_creator = atari.env_creator(env_name)\n",
    "vecenv = pufferlib.vector.make(\n",
    "    env_creator,\n",
    "    num_envs=1,\n",
    "    backend=pufferlib.vector.Serial,\n",
    "    env_kwargs={\"framestack\": 4, \"render_mode\":\"rgb_array\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775b70b-1488-40fe-9ebf-92f9fa5ecbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gymnasium Breakout\n",
    "import ale_py\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f990ecf-bacc-44ac-b6e3-cb599e6e6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "video_folder = f\"./videos/{project}_{timestamp}\"\n",
    "env = gym.wrappers.RecordVideo(env, video_folder)\n",
    "\n",
    "env = gym.wrappers.GrayScaleObservation(env=env)\n",
    "env = gym.wrappers.ResizeObservation(env=env, shape=(105, 80))\n",
    "env = gym.wrappers.FrameStack(env=env, num_stack=4)\n",
    "\n",
    "ob, _ = env.reset()\n",
    "ob = np.expand_dims(np.array(ob).transpose(2, 0, 1), 0)\n",
    "print(ob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7afd5-67c6-445e-92d1-03a120ad2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob, _ = env.reset()\n",
    "ob = np.expand_dims(np.array(ob).transpose(2, 0, 1), 0)\n",
    "\n",
    "ret = 0\n",
    "done, truncated = False, False\n",
    "while not (done or truncated):\n",
    "    action, *_ = select_actions(ob)\n",
    "    ob, reward, done, truncated, _ = env.step(action)\n",
    "    ob = np.expand_dims(np.array(ob).transpose(2, 0, 1), 0)\n",
    "    ret += reward\n",
    "\n",
    "print(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
