{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac974d59-2a7c-4d54-b7ec-39633bed604b",
   "metadata": {},
   "source": [
    "# Testing REINFORCE with Puffer-vectorized Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0c3a0-ac4d-4cc0-bca4-cf6dcb6e7b8e",
   "metadata": {},
   "source": [
    "### Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b350f1c9-d5cf-419b-955b-6cc53ce6db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pufferlib\n",
    "import pufferlib.vector\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from pufferlib.environments import atari\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b870e5-a3c8-4e70-b8b6-df70d1071e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9208ac4-1f70-4d46-ade8-c06fb7ef1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers, for four stacked grayscale 84x84 frames\n",
    "        self.conv1 = nn.Conv2d(in_channels= 4, out_channels=16, kernel_size=5, stride=2) #  4 x 51 x 38\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2) # 16 x 24 x 17\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2) # 32 x 10 x 7\n",
    "        conv_output_size = 32 * 10 * 7\n",
    "\n",
    "        # Fully connected head\n",
    "        self.fc = nn.Linear(conv_output_size, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x / 255.0\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156deb37-b7e2-4bad-ad39-4d88dc3120b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredPolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Linear(121, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b47c179-3f47-4677-9eea-f0e521450e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_actions(obs):\n",
    "    states = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "    logits = policy_net(states)\n",
    "\n",
    "    # Create the distribution and sample from it\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    actions = dist.sample()\n",
    "\n",
    "    # We need the log probability for gradient computation\n",
    "    log_probs = dist.log_prob(actions)\n",
    "\n",
    "    # Get the entropy for logging, possibly as a bonus to the loss\n",
    "    entropies = dist.entropy()\n",
    "\n",
    "    return actions.cpu(), log_probs, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038415d7-a4f7-45dd-8aa0-b3be912c420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    policy: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    env,\n",
    "    update_batches: int = 16,\n",
    "    gamma: float = 0.99,\n",
    "    num_steps: int = 1_000_000,\n",
    "    report_frequency: int = 50,\n",
    "):\n",
    "    try:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        log_probs_list = [[] for _ in range(obs.shape[0])]\n",
    "        entropies_list = [[] for _ in range(obs.shape[0])]\n",
    "        rewards_list = [[] for _ in range(obs.shape[0])]\n",
    "\n",
    "        total_episodes = 0\n",
    "        episodes = 0\n",
    "        returns = torch.zeros(update_batches, device=\"cpu\")\n",
    "        losses = torch.zeros(update_batches, device=device)\n",
    "        \n",
    "        for steps in tqdm(range(1, num_steps + 1), desc=\"Steps\"):\n",
    "            # Use the policy network for action selection\n",
    "            actions, log_probs, current_entropies = select_actions(obs)\n",
    "\n",
    "            # Step the environment with the chosen action\n",
    "            obs, rewards, dones, truncateds, _ = env.step(actions)\n",
    "\n",
    "            # Store log probs and rewards for loss calculation\n",
    "            for i in range(len(rewards)):\n",
    "                log_probs_list[i].append(log_probs[i])\n",
    "                entropies_list[i].append(current_entropies[i])\n",
    "                rewards_list[i].append(rewards[i])\n",
    "\n",
    "            # Handle completed episodes\n",
    "            done_indices = np.where(dones | truncateds)[0]\n",
    "            new_episodes = len(done_indices)\n",
    "            \n",
    "            for ep_i, done_i in enumerate(done_indices):\n",
    "                if episodes + ep_i < update_batches:\n",
    "                    T = len(rewards_list[done_i])\n",
    "                    \n",
    "                    # Compute the return G_t for each time step t\n",
    "                    # and compute the policy gradient loss\n",
    "                    ret = 0.0\n",
    "                    loss = 0.0\n",
    "                    for t in reversed(range(T)):\n",
    "                        ret = rewards_list[done_i][t] + gamma * ret\n",
    "                        loss -= log_probs_list[done_i][t] * ret - entropies_list[done_i][t] * 0.01\n",
    "\n",
    "                    loss /= T\n",
    "                    losses[episodes+ep_i] = loss\n",
    "                    returns[episodes+ep_i] = ret\n",
    "\n",
    "                    if wandb.run is not None:\n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                \"episodes\": total_episodes + ep_i,\n",
    "                                \"ep_length\": T,\n",
    "                                \"return\": ret,\n",
    "                                \"total_reward\": sum(rewards_list[done_i]),\n",
    "                                \"loss\": loss.item(),\n",
    "                            },\n",
    "                            step=total_episodes + ep_i\n",
    "                        )\n",
    "                    \n",
    "                    log_probs_list[done_i] = []\n",
    "                    entropies_list[done_i] = []\n",
    "                    rewards_list[done_i] = []\n",
    "    \n",
    "            total_episodes += new_episodes\n",
    "            episodes += new_episodes\n",
    "                    \n",
    "            if episodes >= update_batches:\n",
    "                avg_return = returns.mean()\n",
    "                avg_loss = losses.mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                avg_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                log_probs_list = [[] for _ in range(obs.shape[0])]\n",
    "                entropies_list = [[] for _ in range(obs.shape[0])]\n",
    "                rewards_list = [[] for _ in range(obs.shape[0])]\n",
    "\n",
    "                episodes = 0\n",
    "                returns = torch.zeros(update_batches, device=\"cpu\")\n",
    "                losses = torch.zeros(update_batches, device=device)\n",
    "                \n",
    "                #obs, _ = env.reset()\n",
    "                \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining stopped manually.\")\n",
    "\n",
    "    if wandb.run is not None:\n",
    "        wandb.unwatch()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0d471-23b4-40be-a70a-b9f7f510049f",
   "metadata": {},
   "source": [
    "## Configuring the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f38c00b-d8e7-4839-a6aa-cc80591a0985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.9.0+750d7f9)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Box(0, 255, (12, 80, 4, 105), uint8),\n",
       " MultiDiscrete([4 4 4 4 4 4 4 4 4 4 4 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = \"Puffer-Breakout-REINFORCE\"\n",
    "env_name = \"breakout\"\n",
    "gamma = 0.99\n",
    "update_batches = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "config = {\n",
    "    \"env\": env_name,\n",
    "    \"algo\": \"REINFORCE\",\n",
    "    \"gamma\": gamma,\n",
    "    \"update_batches\": update_batches,\n",
    "    \"learning_rate\": learning_rate,\n",
    "}\n",
    "\n",
    "env_creator = atari.env_creator(\"breakout\")\n",
    "vecenv = pufferlib.vector.make(\n",
    "    env_creator,\n",
    "    num_envs=12,\n",
    "    backend=pufferlib.vector.Multiprocessing,\n",
    "    env_kwargs={\"framestack\": 4},\n",
    ")\n",
    "\n",
    "vecenv.observation_space, vecenv.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3cef52-526e-4dd9-8069-cd3eb3713c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = vecenv.action_space.nvec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16871cdd-8ea6-4d9e-bc38-a13dc56e2eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork().to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f4890-05c9-4694-b040-56698765ab02",
   "metadata": {},
   "source": [
    "## Initialize W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5160251-9593-4012-9884-c1844d0ef83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "track = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c1cb0e-e0c3-4f3e-a467-ad2124313891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfitti\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fitti/projects/puffer/wandb/run-20250214_180902-xjjo1fn1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE/runs/xjjo1fn1' target=\"_blank\">cute-hug-68</a></strong> to <a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE' target=\"_blank\">https://wandb.ai/fitti/Puffer-Breakout-REINFORCE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fitti/Puffer-Breakout-REINFORCE/runs/xjjo1fn1' target=\"_blank\">https://wandb.ai/fitti/Puffer-Breakout-REINFORCE/runs/xjjo1fn1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if track:\n",
    "    wandb.init(\n",
    "        project=project,\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f086cb55-5647-4d4a-a10a-3b6d0f6fae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(policy_net, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327df5a7-f9b1-41f5-bf30-3bad1333c051",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97549769-b5dc-4a35-afb4-14d595501e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda7f691f5b4f93abb08f5f1dedd11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Steps:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(policy_net, optimizer, vecenv, update_batches=update_batches, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dd201-2167-42f3-a82e-892cc554b6f3",
   "metadata": {},
   "source": [
    "## Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4acad-3a00-4753-b4a2-3bfb9fa32c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_creator = atari.env_creator(env_name)\n",
    "vecenv = pufferlib.vector.make(\n",
    "    env_creator,\n",
    "    num_envs=1,\n",
    "    backend=pufferlib.vector.Serial,\n",
    "    env_kwargs={\"framestack\": 4, \"render_mode\":\"rgb_array\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775b70b-1488-40fe-9ebf-92f9fa5ecbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gymnasium Breakout\n",
    "import ale_py\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f990ecf-bacc-44ac-b6e3-cb599e6e6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "video_folder = f\"./videos/{project}_{timestamp}\"\n",
    "env = gym.wrappers.RecordVideo(env, video_folder)\n",
    "\n",
    "env = gym.wrappers.GrayScaleObservation(env=env)\n",
    "env = gym.wrappers.ResizeObservation(env=env, shape=(105, 80))\n",
    "env = gym.wrappers.FrameStack(env=env, num_stack=4)\n",
    "\n",
    "ob, _ = env.reset()\n",
    "ob = np.expand_dims(np.array(ob).transpose(2, 0, 1), 0)\n",
    "print(ob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7afd5-67c6-445e-92d1-03a120ad2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob, _ = env.reset()\n",
    "ob = np.expand_dims(np.array(ob).transpose(2, 0, 1), 0)\n",
    "\n",
    "ret = 0\n",
    "done, truncated = False, False\n",
    "while not (done or truncated):\n",
    "    action, *_ = select_actions(ob)\n",
    "    ob, reward, done, truncated, _ = env.step(action)\n",
    "    ob = np.expand_dims(np.array(ob).transpose(2, 0, 1), 0)\n",
    "    ret += reward\n",
    "\n",
    "print(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
